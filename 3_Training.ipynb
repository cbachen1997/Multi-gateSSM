{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_Training.ipynb","provenance":[{"file_id":"1ok80Nr0ZCLq0rSafsAsGu496W36y_5Ff","timestamp":1624879397118}],"collapsed_sections":["42_ZFECq_hcG","xMSG66jN7K34","YbR5cUrM7NvN","YUjuVisF-qu3","zc2HR-Hf67z0","urtowHo2CKgo","_JKI0GtiwCE1","SgYTBDASRJKo","gzeK29DiNbDl"],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyPS/ukW2teNo2fwJEcyf6yP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"42_ZFECq_hcG"},"source":["#**基础库**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FWmDYynbz6Lx","executionInfo":{"status":"ok","timestamp":1622557021172,"user_tz":-480,"elapsed":695,"user":{"displayName":"Boan Chen","photoUrl":"","userId":"13891223129202903233"}},"outputId":"019797ab-7100-4a28-b62e-b5aed57af7d2"},"source":["#挂载谷歌云盘\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","####google云盘授权#####\n","##每个notebook执行一次###\n","__author__='CBA'\n","from google.colab import drive\n","\n","#增加PyDrive操作库\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","#授权登录\n","auth.authenticate_user()\n","gauth=GoogleAuth()\n","gauth.credentials=GoogleCredentials.get_application_default()\n","drive=GoogleDrive(gauth)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uC0MW80l1AqJ"},"source":["from __future__ import print_function, division\n","import tensorflow as tf\n","from sklearn.utils import shuffle\n","import tifffile as tiff\n","from tensorflow import keras as K\n","import tensorflow.keras.layers as L\n","import numpy as np\n","import os\n","import time\n","import h5py\n","import argparse \n","import random\n","import cv2\n","from tqdm import *\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.callbacks import LambdaCallback\n","# from keras.callbacks import TensorBoard\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.models import Model\n","from __future__ import print_function, division\n","from tensorflow.keras.layers import *\n","# from keras.layers import Dense,Dropout\n","from sklearn.utils import shuffle\n","# from keras.layers.core import Flatten\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import LeakyReLU\n","# from tensorflow.keras.layers.core import Lambda\n","from tensorflow.keras import backend as Kb\n","from collections import Counter\n","# from utils_1 import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xMSG66jN7K34"},"source":["#基础部分"]},{"cell_type":"code","metadata":{"id":"AxK41v_a-zBL"},"source":["def tf_flatten(a):\n","  \"\"\"Flatten tensor\"\"\"\n","  return tf.reshape(a, [-1])\n","\n","\n","def tf_repeat(a, repeats, axis=0):\n","  \"\"\"TensorFlow version of np.repeat for 1D\"\"\"\n","  # https://github.com/tensorflow/tensorflow/issues/8521\n","  assert len(a.get_shape()) == 1\n","\n","  a = tf.expand_dims(a, -1)\n","  a = tf.tile(a, [1, repeats])\n","  a = tf_flatten(a)\n","  return a\n","\n","\n","def tf_repeat_2d(a, repeats):\n","  \"\"\"Tensorflow version of np.repeat for 2D\"\"\"\n","\n","  assert len(a.get_shape()) == 2\n","  a = tf.expand_dims(a, 0)\n","  a = tf.tile(a, [repeats, 1, 1])\n","  return a\n","\n","\n","def tf_map_coordinates(input, coords, order=1):\n","  \"\"\"Tensorflow verion of scipy.ndimage.map_coordinates\n","\n","  Note that coords is transposed and only 2D is supported\n","\n","  Parameters\n","  ----------\n","  input : tf.Tensor. shape = (s, s)\n","  coords : tf.Tensor. shape = (n_points, 2)\n","  \"\"\"\n","\n","  assert order == 1\n","\n","  coords_lt = tf.cast(tf.floor(coords), 'int32')\n","  #coords_rb = tf.cast(tf.ceil(coords), 'int32')\n","  coords_rb = tf.cast(tf.math.ceil(coords), 'int32')\n","  coords_lb = tf.stack([coords_lt[:, 0], coords_rb[:, 1]], axis=1)\n","  coords_rt = tf.stack([coords_rb[:, 0], coords_lt[:, 1]], axis=1)\n","\n","  vals_lt = tf.gather_nd(input, coords_lt)\n","  vals_rb = tf.gather_nd(input, coords_rb)\n","  vals_lb = tf.gather_nd(input, coords_lb)\n","  vals_rt = tf.gather_nd(input, coords_rt)\n","\n","  coords_offset_lt = coords - tf.cast(coords_lt, 'float32')\n","  vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[:, 0]\n","  vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[:, 0]\n","  mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[:, 1]\n","\n","  return mapped_vals\n","\n","\n","def sp_batch_map_coordinates(inputs, coords):\n","  \"\"\"Reference implementation for batch_map_coordinates\"\"\"\n","  coords = coords.clip(0, inputs.shape[1] - 1)\n","  mapped_vals = np.array([\n","      sp_map_coordinates(input, coord.T, mode='nearest', order=1)\n","      for input, coord in zip(inputs, coords)\n","  ])\n","  return mapped_vals\n","\n","\n","def tf_batch_map_coordinates(input, coords, order=1):\n","  \"\"\"Batch version of tf_map_coordinates\n","\n","  Only supports 2D feature maps\n","\n","  Parameters\n","  ----------\n","  input : tf.Tensor. shape = (b, s, s)\n","  coords : tf.Tensor. shape = (b, n_points, 2)\n","\n","  Returns\n","  -------\n","  tf.Tensor. shape = (b, s, s)\n","  \"\"\"\n","\n","  input_shape = tf.shape(input)\n","  batch_size = input_shape[0]\n","  input_size = input_shape[1]\n","  n_coords = tf.shape(coords)[1]\n","\n","  coords = tf.clip_by_value(coords, 0, tf.cast(input_size, 'float32') - 1)\n","  coords_lt = tf.cast(tf.floor(coords), 'int32')\n","  #coords_rb = tf.cast(tf.ceil(coords), 'int32')\n","  coords_rb = tf.cast(tf.math.ceil(coords), 'int32')\n","  coords_lb = tf.stack([coords_lt[..., 0], coords_rb[..., 1]], axis=-1)\n","  coords_rt = tf.stack([coords_rb[..., 0], coords_lt[..., 1]], axis=-1)\n","\n","  idx = tf_repeat(tf.range(batch_size), n_coords)\n","\n","  def _get_vals_by_coords(input, coords):\n","      indices = tf.stack([\n","          idx, tf_flatten(coords[..., 0]), tf_flatten(coords[..., 1])\n","      ], axis=-1)\n","      vals = tf.gather_nd(input, indices)\n","      vals = tf.reshape(vals, (batch_size, n_coords))\n","      return vals\n","\n","  vals_lt = _get_vals_by_coords(input, coords_lt)\n","  vals_rb = _get_vals_by_coords(input, coords_rb)\n","  vals_lb = _get_vals_by_coords(input, coords_lb)\n","  vals_rt = _get_vals_by_coords(input, coords_rt)\n","\n","  coords_offset_lt = coords - tf.cast(coords_lt, 'float32')\n","  vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[..., 0]\n","  vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[..., 0]\n","  mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[..., 1]\n","\n","  return mapped_vals\n","\n","\n","def sp_batch_map_offsets(input, offsets):\n","  \"\"\"Reference implementation for tf_batch_map_offsets\"\"\"\n","\n","  batch_size = input.shape[0]\n","  input_size = input.shape[1]\n","\n","  offsets = offsets.reshape(batch_size, -1, 2)\n","  grid = np.stack(np.mgrid[:input_size, :input_size], -1).reshape(-1, 2)\n","  grid = np.repeat([grid], batch_size, axis=0)\n","  coords = offsets + grid\n","  coords = coords.clip(0, input_size - 1)\n","\n","  mapped_vals = sp_batch_map_coordinates(input, coords)\n","  return mapped_vals\n","\n","\n","def tf_batch_map_offsets(input, offsets, order=1):\n","  \"\"\"Batch map offsets into input\n","\n","  Parameters\n","  ---------\n","  input : tf.Tensor. shape = (b, s, s)\n","  offsets: tf.Tensor. shape = (b, s, s, 2)\n","\n","  Returns\n","  -------\n","  tf.Tensor. shape = (b, s, s)\n","  \"\"\"\n","\n","  input_shape = tf.shape(input)\n","  batch_size = input_shape[0]\n","  input_size = input_shape[1]\n","\n","  offsets = tf.reshape(offsets, (batch_size, -1, 2))\n","  grid = tf.meshgrid(\n","      tf.range(input_size), tf.range(input_size), indexing='ij'\n","  )\n","  grid = tf.stack(grid, axis=-1)\n","  grid = tf.cast(grid, 'float32')\n","  grid = tf.reshape(grid, (-1, 2))\n","  grid = tf_repeat_2d(grid, batch_size)\n","  coords = offsets + grid\n","\n","  mapped_vals = tf_batch_map_coordinates(input, coords)\n","  return mapped_vals\n","def BN_LeakyReLU(input):\n","    \n","  norm = L.BatchNormalization(axis=-1)(input)\n","  output = L.LeakyReLU(alpha=0.2)(norm)\n","  \n","  return output\n","class ConvOffset2D(Conv2D):\n","  \"\"\"ConvOffset2D\n","\n","  Convolutional layer responsible for learning the 2D offsets and output the\n","  deformed feature map using bilinear interpolation\n","\n","  Note that this layer does not perform convolution on the deformed feature\n","  map. See get_deform_cnn in cnn.py for usage\n","  \"\"\"\n","\n","  def __init__(self, filters, init_normal_stddev=0.01, **kwargs):\n","    \"\"\"Init\n","\n","    Parameters\n","    ----------\n","    filters : int\n","        Number of channel of the input feature map\n","    init_normal_stddev : float\n","        Normal kernel initialization\n","    **kwargs:\n","        Pass to superclass. See Con2D layer in Keras\n","    \"\"\"\n","\n","    self.filters = filters\n","    super(ConvOffset2D, self).__init__(\n","        self.filters * 2, (3, 3), padding='same', use_bias=False,\n","        kernel_initializer=K.initializers.RandomNormal(0, init_normal_stddev),\n","        **kwargs\n","    )\n","\n","  def call(self, x):\n","    \"\"\"Return the deformed featured map\"\"\"\n","    x_shape = x.get_shape()\n","    offsets = super(ConvOffset2D, self).call(x)\n","\n","    # offsets: (b*c, h, w, 2)\n","    offsets = self._to_bc_h_w_2(offsets, x_shape)\n","\n","    # x: (b*c, h, w)\n","    x = self._to_bc_h_w(x, x_shape)\n","\n","    # X_offset: (b*c, h, w)\n","    x_offset = tf_batch_map_offsets(x, offsets)\n","\n","    # x_offset: (b, h, w, c)\n","    x_offset = self._to_b_h_w_c(x_offset, x_shape)\n","\n","    return x_offset\n","\n","  def compute_output_shape(self, input_shape):\n","    \"\"\"Output shape is the same as input shape\n","\n","    Because this layer does only the deformation part\n","    \"\"\"\n","    return input_shape\n","\n","  @staticmethod\n","  def _to_bc_h_w_2(x, x_shape):\n","    \"\"\"(b, h, w, 2c) -> (b*c, h, w, 2)\"\"\"\n","    x = tf.transpose(x, [0, 3, 1, 2])\n","    x = tf.reshape(x, (-1, int(x_shape[1]), int(x_shape[2]), 2))\n","    return x\n","\n","  @staticmethod\n","  def _to_bc_h_w(x, x_shape):\n","    \"\"\"(b, h, w, c) -> (b*c, h, w)\"\"\"\n","    x = tf.transpose(x, [0, 3, 1, 2])\n","    x = tf.reshape(x, (-1, int(x_shape[1]), int(x_shape[2])))\n","    return x\n","\n","  @staticmethod\n","  def _to_b_h_w_c(x, x_shape):\n","    \"\"\"(b*c, h, w) -> (b, h, w, c)\"\"\"\n","    x = tf.reshape(\n","        x, (-1, int(x_shape[3]), int(x_shape[1]), int(x_shape[2]))\n","    )\n","    x = tf.transpose(x, [0, 2, 3, 1])\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YbR5cUrM7NvN"},"source":["#全局参数"]},{"cell_type":"code","metadata":{"id":"W8OR7cqN1Wbl"},"source":["#全局参数\n","hchn = 12\n","r = 7\n","#标签样本数据npy\n","scale=2000#每次抽取的无标签数量构建伪样本池\n","NUM_CLASS = 7"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YUjuVisF-qu3"},"source":["#backbone\n","\n"]},{"cell_type":"code","metadata":{"id":"_e-0ZjFu-rnE"},"source":["######################################backbone部分######################################\n","def feature_extraction_CNN(input_shape, training_parameter= False,n_filters=64):\n","  # X_input=L.Input(input_shape)\n","  conv1 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(input_shape)\n","  \n","  conv1 = BN_LeakyReLU(conv1)\n","  conv1 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(conv1)\n","  conv1 = BN_LeakyReLU(conv1)\n","  \n","  pool1 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv1)\n","  # print(pool1)\n","  #可变形block A1\n","  # print(pool1)\n","  offset_conv2_1 = ConvOffset2D(2*n_filters)(pool1)\n","  \n","  conv2_1 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_1)\n","  conv2_1 = BN_LeakyReLU(conv2_1)\n","  \n","  offset_conv2_2 = ConvOffset2D(n_filters)(conv2_1)\n","  conv2_2 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_2)\n","  conv2_2 = BN_LeakyReLU(conv2_2)\n","  \n","  offset_conv2_3 = ConvOffset2D(int(0.5*n_filters))(conv2_2)\n","  conv2_3 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_3)\n","  conv2_3 = BN_LeakyReLU(conv2_3)\n","  \n","  conv2_4 = L.concatenate([conv2_1, conv2_2, conv2_3], axis=-1)\n","  # print(conv2_4)\n","  conv2_5 = L.add([conv2_4, pool1])\n","\n","  #可变形block A2\n","  offset_conv2_6 = ConvOffset2D(2*n_filters)(conv2_5)\n","  conv2_6 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_6)\n","  conv2_6 = BN_LeakyReLU(conv2_6)\n","  \n","  offset_conv2_7 = ConvOffset2D(n_filters)(conv2_6)\n","  conv2_7 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_7)\n","  conv2_7 = BN_LeakyReLU(conv2_7)\n","  \n","  offset_conv2_8 = ConvOffset2D(int(0.5*n_filters))(conv2_7)\n","  conv2_8 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_8)\n","  conv2_8 = BN_LeakyReLU(conv2_8)\n","  \n","  conv2_9 = L.concatenate([conv2_6, conv2_7, conv2_8], axis=-1)\n","  \n","  conv2_10 = L.add([conv2_9, conv2_5])\n","  \n","  \n","  #pool2 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv2_5)\n","  \n","  conv3 = L.Conv2D(4*n_filters, (3, 3), padding='valid', strides=(2, 2), kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(conv2_5)\n","  \n","  offset_conv3_1 = ConvOffset2D(4*n_filters)(conv3)\n","  \n","  conv3_1 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_1)\n","  conv3_1 = BN_LeakyReLU(conv3_1)\n","  \n","  offset_conv3_2 = ConvOffset2D(2*n_filters)(conv3_1)\n","  conv3_2 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_2)\n","  conv3_2 = BN_LeakyReLU(conv3_2)\n","  offset_conv3_3 = ConvOffset2D(n_filters)(conv3_2)\n","  conv3_3 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_3)\n","  conv3_3 = BN_LeakyReLU(conv3_3)\n","  \n","  conv3_4 = L.concatenate([conv3_1, conv3_2, conv3_3], axis=-1)\n","  \n","  conv3_5 = L.add([conv3_4, conv3])\n","\n","  #改了一下4*\n","  offset_conv3_6 = ConvOffset2D(n_filters)(conv3_2)\n","  \n","  conv3_6 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_6)\n","  conv3_6 = BN_LeakyReLU(conv3_6)\n","  \n","  offset_conv3_7 = ConvOffset2D(2*n_filters)(conv3_6)\n","  conv3_7 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_7)\n","  conv3_7 = BN_LeakyReLU(conv3_7)\n","  \n","  offset_conv3_8 = ConvOffset2D(n_filters)(conv3_7)\n","  conv3_8 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_8)\n","  conv3_8 = BN_LeakyReLU(conv3_8)\n","  \n","  conv3_9 = L.concatenate([conv3_6, conv3_7, conv3_8], axis=-1)\n","  \n","  conv3_10 = L.add([conv3_9, conv3_5])\n","  \n","  \n","  #pool3 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv3)\n","  #输出128维计算度量\n","  # print(conv3_10)\n","  conv4 = L.Conv2D(n_filters*2, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(conv3_10)\n","  # print(conv4)\n","  gap = L.GlobalAvgPool2D()(conv4)\n","  # print(gap)\n","  #增加功能\n","  X = Dense(1024,kernel_regularizer=regularizers.l2(0.01),name='dense_layer1')(gap)\n","  X1 = Dropout(rate=0.5)(X,training=training_parameter)#打开training模式，测试时也使用dropout\n","  X2 = Dense(256,kernel_regularizer=regularizers.l2(0.01),name='dense_layer2')(X1)\n","  X3 = Dropout(rate=0.5)(X2,training=training_parameter)\n","  spatial_result = X3\n","  print('输出向量维度：' + str(spatial_result.shape))\n","  return spatial_result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zc2HR-Hf67z0"},"source":["#🔺模型训练（全监督）"]},{"cell_type":"code","metadata":{"id":"cwY23Ziz1mom"},"source":["#数据读取（自己替换）\n","X_train = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/label_x.npy')\n","Y_train = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/label_y.npy')\n","X_val = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/val_x.npy')\n","Y_val = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/val_y.npy')\n","X_test = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/test_x.npy')\n","Y_test = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/test_y.npy')\n","unlabeled_pool = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/unlabel_x.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n8Nc0K6u8jKX"},"source":["#查看数据集结构\n","# Counter(Y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnbsULZm2mZK"},"source":["def logistic_model(training_para):\n","  ksize = 2 * r + 1\n","  inputshape = L.Input((ksize,ksize,hchn))\n","  # inputshape = (ksize,ksize,hchn)\n","  #返回全局平均池化结果\n","  output_feature = feature_extraction_CNN(inputshape,training_parameter=training_para, n_filters=64)\n","  logits = L.Dense(NUM_CLASS+1, activation = 'softmax')(output_feature)\n","  model = K.models.Model(inputshape, logits)\n","  adam = K.optimizers.Adam(lr=1e-3,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01)\n","  model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics=['acc'])\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GMiuRmbF5x31"},"source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import EarlyStopping\n","def train_logistic(X,Y,X_val,Y_val):\n","  model = logistic_model(training_para=True)\n","  X_train, Y_train = shuffle(X, Y)\n","  \n","  model_checkpoint = ModelCheckpoint('/content/gdrive/MyDrive/CM/RSL/CE_200_M5D12.h5', monitor='val_loss', verbose=1, \n","                                       save_best_only=True, save_weights_only=True,period=10)\n","\n","  early_stopping =EarlyStopping(monitor='val_loss', patience=20)\n","  # print('a')\n","  last_model = model.fit(x=X,y=Y,batch_size=50,epochs=200,shuffle=True,validation_data=(X_val, Y_val),\n","              callbacks=[model_checkpoint, early_stopping])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDE3nqiq58G7"},"source":["Y_train=to_categorical(Y_train)\n","Y_val=to_categorical(Y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1u1CAYm56aX"},"source":["#训练\n","train_logistic(X_train,Y_train,X_val,Y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"urtowHo2CKgo"},"source":["#Softmax门限函数"]},{"cell_type":"code","metadata":{"id":"-8-JVPOf-GX1"},"source":["#判断每一个的softmax概率都大于阈值\n","def softmax_threshold(softmax_result,threshold = 0.9):\n","  max_index = np.argmax(softmax_result,axis=1)\n","  # print(max_index)\n","  count = 0\n","  for i in range(softmax_result.shape[0]):\n","    # print(i)\n","    # print(max_index[i])\n","    if softmax_result[i,max_index[i]] >= threshold:\n","      count += 1\n","  if count == softmax_result.shape[0]:\n","    return True\n","  else:\n","    return count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgpae0HbvXav"},"source":["def get_highsoftmax(model,unlabeled,scale,threshold = 0.9):\n","  temp_unlabeled,temp_index = sampling_unlabel(unlabeled,scale=scale)\n","  temp_predict = model.predict(temp_unlabeled)\n","  temp_predict_max = np.max(temp_predict,axis=1)\n","  #选出大于阈值的样本、和索引及伪标签\n","  temp_predict = temp_predict[temp_predict_max > threshold]\n","  temp_unlabeled = temp_unlabeled[temp_predict_max > threshold]\n","  temp_index = temp_index[temp_predict_max > threshold]\n","  t_plabel = np.argmax(temp_predict,axis=1)\n","  return temp_unlabeled,t_plabel,temp_index\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_JKI0GtiwCE1"},"source":["#高抗噪门限函数\n"]},{"cell_type":"code","metadata":{"id":"We-ZdV5WJiYM"},"source":["def random_crop(image, min_ratio=0.6, max_ratio=1.0):\n","\n","  h, w = image.shape[:2]\n","  \n","  ratio = random.random()\n","  \n","  scale = min_ratio + ratio * (max_ratio - min_ratio)\n","  \n","  new_h = int(h*scale)    \n","  new_w = int(w*scale)\n","  \n","  y = np.random.randint(0, h - new_h)    \n","  x = np.random.randint(0, w - new_w)\n","  \n","  image = image[y:y+new_h, x:x+new_w, :]\n","  \n","  return image\n","def pad(image,pad_shape=(15,15,12)):\n","  dis = pad_shape[0] - image.shape[0]\n","  w1 = random.randint(0,dis)\n","  w2 = dis - w1\n","  # print(w1,w2)\n","  nn = float(np.mean(image))\n","  # print(type(nn))\n","  pad_img = cv2.copyMakeBorder(image, w1, w2, w1, w2, cv2.BORDER_CONSTANT, value=(0,0,0))\n","  # pad_img = np.pad(image[:,:,0:11], ((w1,w2),(w1,w2)),'constant',constant_values=0)\n","  # print(image.shape)\n","  # print(pad_img.shape)\n","  return pad_img\n","def random_crop_pad(image):\n","  temp_img = random_crop(image,min_ratio=0.8,max_ratio=1.0)\n","  temp_img = pad(temp_img)\n","  return temp_img\n","def random_crop_resize(image):\n","  temp_img = random_crop(image,min_ratio=0.6,max_ratio=1.0)\n","  temp_img = cv2.resize(temp_img,(15,15))\n","  return temp_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oGLfxgMKwJ_8"},"source":["#1样本池\n","def sampling_unlabel(unlabeled_X,scale):\n","  pe = np.random.permutation(unlabeled_X.shape[0])\n","  unlabeled_X =unlabeled_X[pe]\n","  if len(unlabeled_X) <= scale:\n","    scale = len(unlabeled_X)\n","  unlabeled_X = unlabeled_X[0:scale,:]\n","  return unlabeled_X, pe[0:scale]\n","#2mixup给伪标签\n","def mixup_supervised(unlabeled,model,scale):\n","  temp_unlabeled,temp_index=sampling_unlabel(unlabeled,scale)\n","  # for i in range(len(temp_unlabeled)):\n","  plabel_all = []\n","  acc_all = []\n","  last_unlabeled = []\n","  last_index = []\n","  print('\\n数据增强中……')\n","  for i in tqdm(range(len(temp_unlabeled)),position=0):\n","    # data_enhanced=np.zeros((7,15,15,12))\n","    #减少增强次数\n","    pseudo_label = []\n","    data_enhanced=np.zeros((10,15,15,12))\n","    data_enhanced[0,]=temp_unlabeled[i,]\n","    #水平垂直翻转\n","    data_enhanced[1,]=np.flip(temp_unlabeled[i,],axis=0)\n","    data_enhanced[2,]=np.flip(temp_unlabeled[i,],axis=1)\n","    #180、270度旋转\n","    data_enhanced[3,]=np.rot90(temp_unlabeled[i,],k=2)\n","    data_enhanced[4,]=np.rot90(temp_unlabeled[i,],k=3)\n","    #增加高斯噪声(轻噪，重噪)\n","    data_enhanced[5,] = cv2.GaussianBlur(temp_unlabeled[i,], (3,3),sigmaX=5,sigmaY=5)\n","    data_enhanced[6,] = cv2.GaussianBlur(temp_unlabeled[i,], (3,3),sigmaX=0.5,sigmaY=0.5)\n","    #随机裁剪缩放\n","    data_enhanced[7,] = random_crop_resize(temp_unlabeled[i,])\n","    #随机裁剪\n","    data_enhanced[8,] = random_crop_pad(temp_unlabeled[i,])\n","    data_enhanced[9,] = random_crop_pad(temp_unlabeled[i,])\n","    ##############################显示\n","    # plt.figure()\n","    # for j in range(len(data_enhanced)):\n","    #   plt.subplot(1,10,j+1)\n","    #   plt.imshow(data_enhanced[j,:,:,0])\n","    # plt.show()\n","    #增强结果的embed\n","    enhance_pred=model.predict(data_enhanced)\n","    #计算度量获取伪标签\n","    if softmax_threshold(enhance_pred,threshold=0.5) == True:\n","      pseudo_label = np.argmax(enhance_pred,axis=1)\n","      # print('都大于')\n","      counts = np.bincount(pseudo_label)\n","      plabel= np.argmax(counts)\n","      acc=counts[plabel]/len(pseudo_label)\n","      plabel_all.append(plabel)\n","      acc_all.append(acc)\n","      last_unlabeled.append(temp_unlabeled[i,])\n","      last_index.append(temp_index[i,])\n","    # else:\n","    #   temp_unlabeled=np.delete(temp_unlabeled,i,axis=0)\n","    #   temp_index=np.delete(temp_index,i)\n","  plabel_all = np.asarray(plabel_all,dtype=int)\n","  acc_all = np.asarray(acc_all,dtype=float)\n","  last_unlabeled = np.asarray(last_unlabeled)\n","  last_index = np.asarray(last_index)\n","  return last_unlabeled,last_index,plabel_all,acc_all"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SgYTBDASRJKo"},"source":["#低不确定性门限函数"]},{"cell_type":"code","metadata":{"id":"0S3MF0MIRPvg"},"source":["def get_uncertainty(h_unlabeled,t_label,h_index,model,uthreshold=0.1,times=100):\n","  uncertain_label = []\n","  all_uncertain_label = []\n","  print('\\n不确定性计算中……')\n","  for i in tqdm(range(times),position=0):\n","    templabel = model.predict(h_unlabeled)\n","    templabel = np.argmax(templabel,axis=1)\n","    uncertain_label.append(templabel)\n","  uncertain_label = np.asarray(uncertain_label)\n","  col_var = np.var(uncertain_label,axis=0)\n","  lu_unlabeled = h_unlabeled[col_var < uthreshold]\n","  lu_index = h_index[col_var < uthreshold]\n","  lu_label = t_label[col_var < uthreshold]\n","  return lu_unlabeled,lu_label,lu_index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VfoRou927mhw"},"source":["#精度评价与混淆矩阵画图\n"]},{"cell_type":"code","metadata":{"id":"HPYJZoVhFgDq"},"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score, cohen_kappa_score\n","def get_OA(cm):\n","    \n","    total = cm.sum()\n","    \n","    diag_arr = np.diagonal(cm, offset=0)\n","    \n","    correct = diag_arr.sum()\n","    \n","    OA = (correct/total) * 100\n","    \n","    return OA\n","\n","def get_cm_oa_kappa(Y_test, Y_pred):\n","    Y_pred = np.argmax(Y_pred, axis=1)\n","    \n","    Y_test = Y_test.tolist()\n","    Y_pred = Y_pred.tolist()\n","  \n","  \n","    cm = confusion_matrix(Y_test,Y_pred)\n","    \n","    oa = get_OA(cm)\n","    \n","    kappa = cohen_kappa_score( Y_test,Y_pred)\n","    \n","    \n","    return cm, oa, kappa\n","\n","def load_npz(npz_path):\n","    \n","    npz_arr = np.load(npz_path)\n","    _files = npz_arr.files\n","    \n","    return npz_arr[_files[0]]\n","\n","import matplotlib.pyplot as pl\n","from sklearn import metrics\n","# 相关库\n","\n","def plot_matrix(y_test, y_pred, labels_name, title=None, thresh=0.8, axis_labels=None):\n","  Y_pred = np.argmax(y_pred, axis=1)\n","  \n","  y_true = y_test.tolist()\n","  Y_pred = Y_pred.tolist()\n","# 利用sklearn中的函数生成混淆矩阵并归一化\n","  cm = confusion_matrix(y_true, Y_pred)  # 生成混淆矩阵 \n","  cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # 归一化\n","\n","# 画图，如果希望改变颜色风格，可以改变此部分的cmap=pl.get_cmap('Blues')处\n","  pl.imshow(cm, interpolation='nearest', cmap=pl.get_cmap('Blues'))\n","  pl.colorbar()  # 绘制图例\n","  pl.title(title, fontsize=15,fontfamily='SimHei')\n","# 图像标题\n","  if title is not None:\n","      pl.title(title)\n","# 绘制坐标\n","  num_local = np.array(range(len(labels_name)))\n","  if axis_labels is None:\n","      axis_labels = labels_name\n","  pl.xticks(num_local, axis_labels, rotation=45, fontsize=10)  # 将标签印在x轴坐标上， 并倾斜45度\n","  pl.yticks(num_local, axis_labels, fontsize=10)  # 将标签印在y轴坐标上\n","  pl.ylabel('True label',fontsize=12,verticalalignment='center')\n","  pl.xlabel('Predicted label',fontsize=12,verticalalignment='center')\n","  \n","\n","# 将百分比打印在相应的格子内，大于thresh的用白字，小于的用黑字\n","  for i in range(np.shape(cm)[0]):\n","      for j in range(np.shape(cm)[1]):\n","          if int(cm[i][j] * 100 + 0.5) > 0:\n","              pl.text(j, i, format(int(cm[i][j] * 100), 'd') + '%',\n","                      ha=\"center\", va=\"center\",\n","                      color=\"white\" if cm[i][j] > thresh else \"black\")  # 如果要更改颜色风格，需要同时更改此行\n","# 显示\n","  # pl.figure(dpi=1000) \n","  pl.savefig('/content/gdrive/MyDrive/CM/RSL/paperimage/cm_2.png',dpi=1000,bbox_inches = 'tight')\n","  # pl.savefig('/content/gdrive/MyDrive/CM/RSL/paperimage/cm.png')\n","  pl.show()\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2n4wSfTXiZsC"},"source":["labels = ['farmland', 'forest', 'grassland', 'water', 'built-up', 'bare land', 'others']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJRHzQL9npFW"},"source":["model1 = import_model('/content/gdrive/MyDrive/CM/RSL/CE_highsoftmx_highmix_lowups4.h5',train_para = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JRaAnPt66Vi6","executionInfo":{"status":"ok","timestamp":1622557161467,"user_tz":-480,"elapsed":5644,"user":{"displayName":"Boan Chen","photoUrl":"","userId":"13891223129202903233"}},"outputId":"06493e24-b2ad-4b02-c0f3-87608f55de26"},"source":["Y_pred = model1.predict(X_test, verbose=1)\n","# print(Y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["650/650 [==============================] - 6s 9ms/sample\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0D6DjUDQ_UzC","executionInfo":{"status":"ok","timestamp":1622557168284,"user_tz":-480,"elapsed":365,"user":{"displayName":"Boan Chen","photoUrl":"","userId":"13891223129202903233"}},"outputId":"291831da-f4f9-4604-8ea1-f020a04b7093"},"source":["cm, oa, kappa = get_cm_oa_kappa(Y_test, Y_pred)\n","print('acc: {:.2f}%  Kappa: {:.4f}'.format(oa,kappa))\n","print('acc: {:2f}%'.format(oa))\n","print(cm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["acc: 91.23%  Kappa: 0.8970\n","acc: 91.230769%\n","[[99  0  0  0  0  1  0]\n"," [ 1 86 12  1  0  0  0]\n"," [ 1  5 90  0  0  4  0]\n"," [ 0  0  0 94  0  6  0]\n"," [ 0  0  3  0 86 11  0]\n"," [ 0  0  3  0  1 96  0]\n"," [ 0  1  5  1  0  1 42]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B9mKI9-Jcj95","outputId":"00d6939e-dc90-4531-a97c-1e7f468abd45"},"source":["plot_matrix(Y_test ,Y_pred, [0,1,2,3,4,5,6], title='Confusion Matrix',\n","            axis_labels=labels)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["findfont: Font family ['SimHei'] not found. Falling back to DejaVu Sans.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"gzeK29DiNbDl"},"source":["#半监督函数"]},{"cell_type":"code","metadata":{"id":"jHIpujC5LsSV"},"source":["def delete_unlabeled(u_pool,d_index):\n","  # for i in d_index:\n","  d_pool = np.delete(u_pool,d_index,axis=0)\n","  return d_pool\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DD5lT5EauAvy"},"source":["def import_model(weightpath,train_para):\n","  ksize = 2 * r + 1\n","  inputshape = L.Input((ksize,ksize,hchn))\n","  output_feature = feature_extraction_CNN(inputshape,training_parameter=train_para, n_filters=64)\n","  logits = L.Dense(NUM_CLASS+1, activation = 'softmax')(output_feature)\n","  model = K.models.Model(inputshape, logits)\n","  adam = K.optimizers.Adam(lr=1e-4,beta_1=0.9, beta_2=0.999, epsilon=None,decay=0.01)\n","  # adam = K.optimizers.Adam(lr=1e-6)\n","  model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics=['acc'])\n","  model.load_weights(weightpath)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utGY56ePrLeg"},"source":["# #限定增加伪标签样本个数\n","def sampling_unlabel(unlabeled_X,scale):\n","  pe = np.random.permutation(unlabeled_X.shape[0])\n","  unlabeled_X =unlabeled_X[pe]\n","  if len(unlabeled_X) <= scale:\n","    scale = len(unlabeled_X)\n","  unlabeled_X = unlabeled_X[0:scale,:]\n","  return unlabeled_X, pe[0:scale]\n","def limited_pseudo_label(sample,plabel,index,lnumber=50):\n","  \"\"\"\n","  lnumber:限制的单类最大个数\n","  \"\"\"\n","  #逐类确认样本个数\n","  least_num = Counter(plabel).most_common()[-1][1]\n","  # print(least_num)\n","  flag = 0\n","  for i in range(1,8):\n","    ti = index[plabel == i]\n","    ts = sample[plabel == i]\n","    tl = plabel[plabel == i]\n","    # print(tl)\n","    if len(ti) >= lnumber:\n","      randnum = random.sample(range(len(ti)),lnumber)\n","    else:\n","      randnum = [j for j in range(len(ti))]\n","    # print(randnum)\n","    ti = ti[randnum]\n","    ts = ts[randnum,:]\n","    tl = tl[randnum]\n","    if flag == 0:\n","      last_sample = ts\n","      last_label = tl\n","      last_index = ti\n","      flag += 1\n","    else:\n","      last_sample = np.append(last_sample,ts,axis=0)\n","      last_label = np.append(last_label,tl)\n","      last_index = np.append(last_index,ti)\n","  return last_sample,last_label,last_index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GiZ8UXSA4KMz"},"source":["#softmax高置信度+高抗噪性+UPS（三门控半监督）"]},{"cell_type":"code","metadata":{"id":"F7FUd_n3QwJA"},"source":["def train_hs_hm_lups(X,Y,X_val,Y_val,unlabeled_pool):\n","  ####设置参数\n","  num_times = 5#抽取num_times次，每次epoch轮\n","  X_train, Y_train = shuffle(X, Y)\n","  model_checkpoint = ModelCheckpoint('/content/gdrive/MyDrive/CM/RSL/CE_highsoftmx_highmix_lowups6.h5', monitor='val_acc', verbose=1, \n","                                       save_best_only=True, save_weights_only=True,period=3)\n","  early_stopping =EarlyStopping(monitor='val_acc', patience=20)\n","####载入初始权重，训练模式\n"," \n","  last_X = X\n","  last_Y = Y\n","  flag = 0\n","\n","  for i in tqdm(range(num_times),position=0):\n","    #先选出高置信度\n","    mmodel = import_model('/content/gdrive/MyDrive/CM/RSL/CE_hightsoftmax&low_UPS.h5',train_para=False)\n","    temp_unlabel,temp_plabel,temp_index = get_highsoftmax(mmodel,unlabeled_pool,5000,threshold = 0.98)\n","    #再计算高抗噪性\n","    temp_unlabel,temp_plabel,temp_index = highmix_unlabeled_for_softmax(temp_unlabel,temp_index,temp_plabel,mmodel,threshold=0.9)\n","    #然后计算低不确定性，打开dropout\n","    mmodel = import_model('/content/gdrive/MyDrive/CM/RSL/CE_hightsoftmax&low_UPS.h5',train_para=True)\n","    temp_unlabel,temp_plabel,temp_index = unlabel2lowups(temp_unlabel,temp_index,temp_plabel,mmodel,uthreshold=0.1,times=100)\n","    print('初始训练集大小：',X.shape)\n","    # print(Counter(temp_plabel))\n","    temp_unlabel,temp_plabel,temp_index = limited_pseudo_label(temp_unlabel,temp_plabel,temp_index)#平衡抽样\n","    print('新增样本统计:',Counter(temp_plabel))\n","    unlabeled_pool = delete_unlabeled(unlabeled_pool,temp_index)#更新样本池\n","    temp_plabel = to_categorical(temp_plabel,8)#同类型\n","    # print(last_Y.shape)\n","    last_X = np.append(last_X,temp_unlabel,axis=0)\n","    last_Y = np.append(last_Y,temp_plabel,axis=0)\n","    print('增加厚训练集大小：',last_X.shape)\n","    # print(last_Y.shape)\n","    mmodel.fit(x=last_X,y=last_Y,batch_size=50,epochs=100,shuffle=True,validation_data=(X_val, Y_val),\n","                        callbacks=[model_checkpoint, early_stopping])\n","    flag += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9sqeWU6_VBWd"},"source":["Y_train=to_categorical(Y_train)\n","Y_val=to_categorical(Y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4g8XhWSUGMq"},"source":["train_hs_hm_lups(X_train,Y_train,X_val,Y_val,unlabeled_pool)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETMgh45ETopq"},"source":[""],"execution_count":null,"outputs":[]}]}