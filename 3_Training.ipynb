{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_Training.ipynb","provenance":[{"file_id":"1ok80Nr0ZCLq0rSafsAsGu496W36y_5Ff","timestamp":1624879397118}],"collapsed_sections":["42_ZFECq_hcG","xMSG66jN7K34","YbR5cUrM7NvN","YUjuVisF-qu3","zc2HR-Hf67z0","urtowHo2CKgo","_JKI0GtiwCE1","SgYTBDASRJKo","gzeK29DiNbDl"],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyPS/ukW2teNo2fwJEcyf6yP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"42_ZFECq_hcG"},"source":["#**åŸºç¡€åº“**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FWmDYynbz6Lx","executionInfo":{"status":"ok","timestamp":1622557021172,"user_tz":-480,"elapsed":695,"user":{"displayName":"Boan Chen","photoUrl":"","userId":"13891223129202903233"}},"outputId":"019797ab-7100-4a28-b62e-b5aed57af7d2"},"source":["#æŒ‚è½½è°·æ­Œäº‘ç›˜\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","####googleäº‘ç›˜æˆæƒ#####\n","##æ¯ä¸ªnotebookæ‰§è¡Œä¸€æ¬¡###\n","__author__='CBA'\n","from google.colab import drive\n","\n","#å¢åŠ PyDriveæ“ä½œåº“\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","#æˆæƒç™»å½•\n","auth.authenticate_user()\n","gauth=GoogleAuth()\n","gauth.credentials=GoogleCredentials.get_application_default()\n","drive=GoogleDrive(gauth)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uC0MW80l1AqJ"},"source":["from __future__ import print_function, division\n","import tensorflow as tf\n","from sklearn.utils import shuffle\n","import tifffile as tiff\n","from tensorflow import keras as K\n","import tensorflow.keras.layers as L\n","import numpy as np\n","import os\n","import time\n","import h5py\n","import argparse \n","import random\n","import cv2\n","from tqdm import *\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.callbacks import LambdaCallback\n","# from keras.callbacks import TensorBoard\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.models import Model\n","from __future__ import print_function, division\n","from tensorflow.keras.layers import *\n","# from keras.layers import Dense,Dropout\n","from sklearn.utils import shuffle\n","# from keras.layers.core import Flatten\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import LeakyReLU\n","# from tensorflow.keras.layers.core import Lambda\n","from tensorflow.keras import backend as Kb\n","from collections import Counter\n","# from utils_1 import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xMSG66jN7K34"},"source":["#åŸºç¡€éƒ¨åˆ†"]},{"cell_type":"code","metadata":{"id":"AxK41v_a-zBL"},"source":["def tf_flatten(a):\n","  \"\"\"Flatten tensor\"\"\"\n","  return tf.reshape(a, [-1])\n","\n","\n","def tf_repeat(a, repeats, axis=0):\n","  \"\"\"TensorFlow version of np.repeat for 1D\"\"\"\n","  # https://github.com/tensorflow/tensorflow/issues/8521\n","  assert len(a.get_shape()) == 1\n","\n","  a = tf.expand_dims(a, -1)\n","  a = tf.tile(a, [1, repeats])\n","  a = tf_flatten(a)\n","  return a\n","\n","\n","def tf_repeat_2d(a, repeats):\n","  \"\"\"Tensorflow version of np.repeat for 2D\"\"\"\n","\n","  assert len(a.get_shape()) == 2\n","  a = tf.expand_dims(a, 0)\n","  a = tf.tile(a, [repeats, 1, 1])\n","  return a\n","\n","\n","def tf_map_coordinates(input, coords, order=1):\n","  \"\"\"Tensorflow verion of scipy.ndimage.map_coordinates\n","\n","  Note that coords is transposed and only 2D is supported\n","\n","  Parameters\n","  ----------\n","  input : tf.Tensor. shape = (s, s)\n","  coords : tf.Tensor. shape = (n_points, 2)\n","  \"\"\"\n","\n","  assert order == 1\n","\n","  coords_lt = tf.cast(tf.floor(coords), 'int32')\n","  #coords_rb = tf.cast(tf.ceil(coords), 'int32')\n","  coords_rb = tf.cast(tf.math.ceil(coords), 'int32')\n","  coords_lb = tf.stack([coords_lt[:, 0], coords_rb[:, 1]], axis=1)\n","  coords_rt = tf.stack([coords_rb[:, 0], coords_lt[:, 1]], axis=1)\n","\n","  vals_lt = tf.gather_nd(input, coords_lt)\n","  vals_rb = tf.gather_nd(input, coords_rb)\n","  vals_lb = tf.gather_nd(input, coords_lb)\n","  vals_rt = tf.gather_nd(input, coords_rt)\n","\n","  coords_offset_lt = coords - tf.cast(coords_lt, 'float32')\n","  vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[:, 0]\n","  vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[:, 0]\n","  mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[:, 1]\n","\n","  return mapped_vals\n","\n","\n","def sp_batch_map_coordinates(inputs, coords):\n","  \"\"\"Reference implementation for batch_map_coordinates\"\"\"\n","  coords = coords.clip(0, inputs.shape[1] - 1)\n","  mapped_vals = np.array([\n","      sp_map_coordinates(input, coord.T, mode='nearest', order=1)\n","      for input, coord in zip(inputs, coords)\n","  ])\n","  return mapped_vals\n","\n","\n","def tf_batch_map_coordinates(input, coords, order=1):\n","  \"\"\"Batch version of tf_map_coordinates\n","\n","  Only supports 2D feature maps\n","\n","  Parameters\n","  ----------\n","  input : tf.Tensor. shape = (b, s, s)\n","  coords : tf.Tensor. shape = (b, n_points, 2)\n","\n","  Returns\n","  -------\n","  tf.Tensor. shape = (b, s, s)\n","  \"\"\"\n","\n","  input_shape = tf.shape(input)\n","  batch_size = input_shape[0]\n","  input_size = input_shape[1]\n","  n_coords = tf.shape(coords)[1]\n","\n","  coords = tf.clip_by_value(coords, 0, tf.cast(input_size, 'float32') - 1)\n","  coords_lt = tf.cast(tf.floor(coords), 'int32')\n","  #coords_rb = tf.cast(tf.ceil(coords), 'int32')\n","  coords_rb = tf.cast(tf.math.ceil(coords), 'int32')\n","  coords_lb = tf.stack([coords_lt[..., 0], coords_rb[..., 1]], axis=-1)\n","  coords_rt = tf.stack([coords_rb[..., 0], coords_lt[..., 1]], axis=-1)\n","\n","  idx = tf_repeat(tf.range(batch_size), n_coords)\n","\n","  def _get_vals_by_coords(input, coords):\n","      indices = tf.stack([\n","          idx, tf_flatten(coords[..., 0]), tf_flatten(coords[..., 1])\n","      ], axis=-1)\n","      vals = tf.gather_nd(input, indices)\n","      vals = tf.reshape(vals, (batch_size, n_coords))\n","      return vals\n","\n","  vals_lt = _get_vals_by_coords(input, coords_lt)\n","  vals_rb = _get_vals_by_coords(input, coords_rb)\n","  vals_lb = _get_vals_by_coords(input, coords_lb)\n","  vals_rt = _get_vals_by_coords(input, coords_rt)\n","\n","  coords_offset_lt = coords - tf.cast(coords_lt, 'float32')\n","  vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[..., 0]\n","  vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[..., 0]\n","  mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[..., 1]\n","\n","  return mapped_vals\n","\n","\n","def sp_batch_map_offsets(input, offsets):\n","  \"\"\"Reference implementation for tf_batch_map_offsets\"\"\"\n","\n","  batch_size = input.shape[0]\n","  input_size = input.shape[1]\n","\n","  offsets = offsets.reshape(batch_size, -1, 2)\n","  grid = np.stack(np.mgrid[:input_size, :input_size], -1).reshape(-1, 2)\n","  grid = np.repeat([grid], batch_size, axis=0)\n","  coords = offsets + grid\n","  coords = coords.clip(0, input_size - 1)\n","\n","  mapped_vals = sp_batch_map_coordinates(input, coords)\n","  return mapped_vals\n","\n","\n","def tf_batch_map_offsets(input, offsets, order=1):\n","  \"\"\"Batch map offsets into input\n","\n","  Parameters\n","  ---------\n","  input : tf.Tensor. shape = (b, s, s)\n","  offsets: tf.Tensor. shape = (b, s, s, 2)\n","\n","  Returns\n","  -------\n","  tf.Tensor. shape = (b, s, s)\n","  \"\"\"\n","\n","  input_shape = tf.shape(input)\n","  batch_size = input_shape[0]\n","  input_size = input_shape[1]\n","\n","  offsets = tf.reshape(offsets, (batch_size, -1, 2))\n","  grid = tf.meshgrid(\n","      tf.range(input_size), tf.range(input_size), indexing='ij'\n","  )\n","  grid = tf.stack(grid, axis=-1)\n","  grid = tf.cast(grid, 'float32')\n","  grid = tf.reshape(grid, (-1, 2))\n","  grid = tf_repeat_2d(grid, batch_size)\n","  coords = offsets + grid\n","\n","  mapped_vals = tf_batch_map_coordinates(input, coords)\n","  return mapped_vals\n","def BN_LeakyReLU(input):\n","    \n","  norm = L.BatchNormalization(axis=-1)(input)\n","  output = L.LeakyReLU(alpha=0.2)(norm)\n","  \n","  return output\n","class ConvOffset2D(Conv2D):\n","  \"\"\"ConvOffset2D\n","\n","  Convolutional layer responsible for learning the 2D offsets and output the\n","  deformed feature map using bilinear interpolation\n","\n","  Note that this layer does not perform convolution on the deformed feature\n","  map. See get_deform_cnn in cnn.py for usage\n","  \"\"\"\n","\n","  def __init__(self, filters, init_normal_stddev=0.01, **kwargs):\n","    \"\"\"Init\n","\n","    Parameters\n","    ----------\n","    filters : int\n","        Number of channel of the input feature map\n","    init_normal_stddev : float\n","        Normal kernel initialization\n","    **kwargs:\n","        Pass to superclass. See Con2D layer in Keras\n","    \"\"\"\n","\n","    self.filters = filters\n","    super(ConvOffset2D, self).__init__(\n","        self.filters * 2, (3, 3), padding='same', use_bias=False,\n","        kernel_initializer=K.initializers.RandomNormal(0, init_normal_stddev),\n","        **kwargs\n","    )\n","\n","  def call(self, x):\n","    \"\"\"Return the deformed featured map\"\"\"\n","    x_shape = x.get_shape()\n","    offsets = super(ConvOffset2D, self).call(x)\n","\n","    # offsets: (b*c, h, w, 2)\n","    offsets = self._to_bc_h_w_2(offsets, x_shape)\n","\n","    # x: (b*c, h, w)\n","    x = self._to_bc_h_w(x, x_shape)\n","\n","    # X_offset: (b*c, h, w)\n","    x_offset = tf_batch_map_offsets(x, offsets)\n","\n","    # x_offset: (b, h, w, c)\n","    x_offset = self._to_b_h_w_c(x_offset, x_shape)\n","\n","    return x_offset\n","\n","  def compute_output_shape(self, input_shape):\n","    \"\"\"Output shape is the same as input shape\n","\n","    Because this layer does only the deformation part\n","    \"\"\"\n","    return input_shape\n","\n","  @staticmethod\n","  def _to_bc_h_w_2(x, x_shape):\n","    \"\"\"(b, h, w, 2c) -> (b*c, h, w, 2)\"\"\"\n","    x = tf.transpose(x, [0, 3, 1, 2])\n","    x = tf.reshape(x, (-1, int(x_shape[1]), int(x_shape[2]), 2))\n","    return x\n","\n","  @staticmethod\n","  def _to_bc_h_w(x, x_shape):\n","    \"\"\"(b, h, w, c) -> (b*c, h, w)\"\"\"\n","    x = tf.transpose(x, [0, 3, 1, 2])\n","    x = tf.reshape(x, (-1, int(x_shape[1]), int(x_shape[2])))\n","    return x\n","\n","  @staticmethod\n","  def _to_b_h_w_c(x, x_shape):\n","    \"\"\"(b*c, h, w) -> (b, h, w, c)\"\"\"\n","    x = tf.reshape(\n","        x, (-1, int(x_shape[3]), int(x_shape[1]), int(x_shape[2]))\n","    )\n","    x = tf.transpose(x, [0, 2, 3, 1])\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YbR5cUrM7NvN"},"source":["#å…¨å±€å‚æ•°"]},{"cell_type":"code","metadata":{"id":"W8OR7cqN1Wbl"},"source":["#å…¨å±€å‚æ•°\n","hchn = 12\n","r = 7\n","#æ ‡ç­¾æ ·æœ¬æ•°æ®npy\n","scale=2000#æ¯æ¬¡æŠ½å–çš„æ— æ ‡ç­¾æ•°é‡æ„å»ºä¼ªæ ·æœ¬æ± \n","NUM_CLASS = 7"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YUjuVisF-qu3"},"source":["#backbone\n","\n"]},{"cell_type":"code","metadata":{"id":"_e-0ZjFu-rnE"},"source":["######################################backboneéƒ¨åˆ†######################################\n","def feature_extraction_CNN(input_shape, training_parameter= False,n_filters=64):\n","  # X_input=L.Input(input_shape)\n","  conv1 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(input_shape)\n","  \n","  conv1 = BN_LeakyReLU(conv1)\n","  conv1 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(conv1)\n","  conv1 = BN_LeakyReLU(conv1)\n","  \n","  pool1 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv1)\n","  # print(pool1)\n","  #å¯å˜å½¢block A1\n","  # print(pool1)\n","  offset_conv2_1 = ConvOffset2D(2*n_filters)(pool1)\n","  \n","  conv2_1 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_1)\n","  conv2_1 = BN_LeakyReLU(conv2_1)\n","  \n","  offset_conv2_2 = ConvOffset2D(n_filters)(conv2_1)\n","  conv2_2 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_2)\n","  conv2_2 = BN_LeakyReLU(conv2_2)\n","  \n","  offset_conv2_3 = ConvOffset2D(int(0.5*n_filters))(conv2_2)\n","  conv2_3 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_3)\n","  conv2_3 = BN_LeakyReLU(conv2_3)\n","  \n","  conv2_4 = L.concatenate([conv2_1, conv2_2, conv2_3], axis=-1)\n","  # print(conv2_4)\n","  conv2_5 = L.add([conv2_4, pool1])\n","\n","  #å¯å˜å½¢block A2\n","  offset_conv2_6 = ConvOffset2D(2*n_filters)(conv2_5)\n","  conv2_6 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_6)\n","  conv2_6 = BN_LeakyReLU(conv2_6)\n","  \n","  offset_conv2_7 = ConvOffset2D(n_filters)(conv2_6)\n","  conv2_7 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_7)\n","  conv2_7 = BN_LeakyReLU(conv2_7)\n","  \n","  offset_conv2_8 = ConvOffset2D(int(0.5*n_filters))(conv2_7)\n","  conv2_8 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_8)\n","  conv2_8 = BN_LeakyReLU(conv2_8)\n","  \n","  conv2_9 = L.concatenate([conv2_6, conv2_7, conv2_8], axis=-1)\n","  \n","  conv2_10 = L.add([conv2_9, conv2_5])\n","  \n","  \n","  #pool2 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv2_5)\n","  \n","  conv3 = L.Conv2D(4*n_filters, (3, 3), padding='valid', strides=(2, 2), kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(conv2_5)\n","  \n","  offset_conv3_1 = ConvOffset2D(4*n_filters)(conv3)\n","  \n","  conv3_1 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_1)\n","  conv3_1 = BN_LeakyReLU(conv3_1)\n","  \n","  offset_conv3_2 = ConvOffset2D(2*n_filters)(conv3_1)\n","  conv3_2 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_2)\n","  conv3_2 = BN_LeakyReLU(conv3_2)\n","  offset_conv3_3 = ConvOffset2D(n_filters)(conv3_2)\n","  conv3_3 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_3)\n","  conv3_3 = BN_LeakyReLU(conv3_3)\n","  \n","  conv3_4 = L.concatenate([conv3_1, conv3_2, conv3_3], axis=-1)\n","  \n","  conv3_5 = L.add([conv3_4, conv3])\n","\n","  #æ”¹äº†ä¸€ä¸‹4*\n","  offset_conv3_6 = ConvOffset2D(n_filters)(conv3_2)\n","  \n","  conv3_6 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_6)\n","  conv3_6 = BN_LeakyReLU(conv3_6)\n","  \n","  offset_conv3_7 = ConvOffset2D(2*n_filters)(conv3_6)\n","  conv3_7 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_7)\n","  conv3_7 = BN_LeakyReLU(conv3_7)\n","  \n","  offset_conv3_8 = ConvOffset2D(n_filters)(conv3_7)\n","  conv3_8 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_8)\n","  conv3_8 = BN_LeakyReLU(conv3_8)\n","  \n","  conv3_9 = L.concatenate([conv3_6, conv3_7, conv3_8], axis=-1)\n","  \n","  conv3_10 = L.add([conv3_9, conv3_5])\n","  \n","  \n","  #pool3 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv3)\n","  #è¾“å‡º128ç»´è®¡ç®—åº¦é‡\n","  # print(conv3_10)\n","  conv4 = L.Conv2D(n_filters*2, (3, 3), padding='same', kernel_initializer='he_normal',\n","                  kernel_regularizer=regularizers.l2(0.0001))(conv3_10)\n","  # print(conv4)\n","  gap = L.GlobalAvgPool2D()(conv4)\n","  # print(gap)\n","  #å¢åŠ åŠŸèƒ½\n","  X = Dense(1024,kernel_regularizer=regularizers.l2(0.01),name='dense_layer1')(gap)\n","  X1 = Dropout(rate=0.5)(X,training=training_parameter)#æ‰“å¼€trainingæ¨¡å¼ï¼Œæµ‹è¯•æ—¶ä¹Ÿä½¿ç”¨dropout\n","  X2 = Dense(256,kernel_regularizer=regularizers.l2(0.01),name='dense_layer2')(X1)\n","  X3 = Dropout(rate=0.5)(X2,training=training_parameter)\n","  spatial_result = X3\n","  print('è¾“å‡ºå‘é‡ç»´åº¦ï¼š' + str(spatial_result.shape))\n","  return spatial_result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zc2HR-Hf67z0"},"source":["#ğŸ”ºæ¨¡å‹è®­ç»ƒï¼ˆå…¨ç›‘ç£ï¼‰"]},{"cell_type":"code","metadata":{"id":"cwY23Ziz1mom"},"source":["#æ•°æ®è¯»å–ï¼ˆè‡ªå·±æ›¿æ¢ï¼‰\n","X_train = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/label_x.npy')\n","Y_train = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/label_y.npy')\n","X_val = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/val_x.npy')\n","Y_val = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/val_y.npy')\n","X_test = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/test_x.npy')\n","Y_test = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/test_y.npy')\n","unlabeled_pool = np.load('/content/gdrive/MyDrive/CM/RSL/RSL_dataset/unlabel_x.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n8Nc0K6u8jKX"},"source":["#æŸ¥çœ‹æ•°æ®é›†ç»“æ„\n","# Counter(Y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnbsULZm2mZK"},"source":["def logistic_model(training_para):\n","  ksize = 2 * r + 1\n","  inputshape = L.Input((ksize,ksize,hchn))\n","  # inputshape = (ksize,ksize,hchn)\n","  #è¿”å›å…¨å±€å¹³å‡æ± åŒ–ç»“æœ\n","  output_feature = feature_extraction_CNN(inputshape,training_parameter=training_para, n_filters=64)\n","  logits = L.Dense(NUM_CLASS+1, activation = 'softmax')(output_feature)\n","  model = K.models.Model(inputshape, logits)\n","  adam = K.optimizers.Adam(lr=1e-3,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01)\n","  model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics=['acc'])\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GMiuRmbF5x31"},"source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import EarlyStopping\n","def train_logistic(X,Y,X_val,Y_val):\n","  model = logistic_model(training_para=True)\n","  X_train, Y_train = shuffle(X, Y)\n","  \n","  model_checkpoint = ModelCheckpoint('/content/gdrive/MyDrive/CM/RSL/CE_200_M5D12.h5', monitor='val_loss', verbose=1, \n","                                       save_best_only=True, save_weights_only=True,period=10)\n","\n","  early_stopping =EarlyStopping(monitor='val_loss', patience=20)\n","  # print('a')\n","  last_model = model.fit(x=X,y=Y,batch_size=50,epochs=200,shuffle=True,validation_data=(X_val, Y_val),\n","              callbacks=[model_checkpoint, early_stopping])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDE3nqiq58G7"},"source":["Y_train=to_categorical(Y_train)\n","Y_val=to_categorical(Y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1u1CAYm56aX"},"source":["#è®­ç»ƒ\n","train_logistic(X_train,Y_train,X_val,Y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"urtowHo2CKgo"},"source":["#Softmaxé—¨é™å‡½æ•°"]},{"cell_type":"code","metadata":{"id":"-8-JVPOf-GX1"},"source":["#åˆ¤æ–­æ¯ä¸€ä¸ªçš„softmaxæ¦‚ç‡éƒ½å¤§äºé˜ˆå€¼\n","def softmax_threshold(softmax_result,threshold = 0.9):\n","  max_index = np.argmax(softmax_result,axis=1)\n","  # print(max_index)\n","  count = 0\n","  for i in range(softmax_result.shape[0]):\n","    # print(i)\n","    # print(max_index[i])\n","    if softmax_result[i,max_index[i]] >= threshold:\n","      count += 1\n","  if count == softmax_result.shape[0]:\n","    return True\n","  else:\n","    return count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgpae0HbvXav"},"source":["def get_highsoftmax(model,unlabeled,scale,threshold = 0.9):\n","  temp_unlabeled,temp_index = sampling_unlabel(unlabeled,scale=scale)\n","  temp_predict = model.predict(temp_unlabeled)\n","  temp_predict_max = np.max(temp_predict,axis=1)\n","  #é€‰å‡ºå¤§äºé˜ˆå€¼çš„æ ·æœ¬ã€å’Œç´¢å¼•åŠä¼ªæ ‡ç­¾\n","  temp_predict = temp_predict[temp_predict_max > threshold]\n","  temp_unlabeled = temp_unlabeled[temp_predict_max > threshold]\n","  temp_index = temp_index[temp_predict_max > threshold]\n","  t_plabel = np.argmax(temp_predict,axis=1)\n","  return temp_unlabeled,t_plabel,temp_index\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_JKI0GtiwCE1"},"source":["#é«˜æŠ—å™ªé—¨é™å‡½æ•°\n"]},{"cell_type":"code","metadata":{"id":"We-ZdV5WJiYM"},"source":["def random_crop(image, min_ratio=0.6, max_ratio=1.0):\n","\n","  h, w = image.shape[:2]\n","  \n","  ratio = random.random()\n","  \n","  scale = min_ratio + ratio * (max_ratio - min_ratio)\n","  \n","  new_h = int(h*scale)    \n","  new_w = int(w*scale)\n","  \n","  y = np.random.randint(0, h - new_h)    \n","  x = np.random.randint(0, w - new_w)\n","  \n","  image = image[y:y+new_h, x:x+new_w, :]\n","  \n","  return image\n","def pad(image,pad_shape=(15,15,12)):\n","  dis = pad_shape[0] - image.shape[0]\n","  w1 = random.randint(0,dis)\n","  w2 = dis - w1\n","  # print(w1,w2)\n","  nn = float(np.mean(image))\n","  # print(type(nn))\n","  pad_img = cv2.copyMakeBorder(image, w1, w2, w1, w2, cv2.BORDER_CONSTANT, value=(0,0,0))\n","  # pad_img = np.pad(image[:,:,0:11], ((w1,w2),(w1,w2)),'constant',constant_values=0)\n","  # print(image.shape)\n","  # print(pad_img.shape)\n","  return pad_img\n","def random_crop_pad(image):\n","  temp_img = random_crop(image,min_ratio=0.8,max_ratio=1.0)\n","  temp_img = pad(temp_img)\n","  return temp_img\n","def random_crop_resize(image):\n","  temp_img = random_crop(image,min_ratio=0.6,max_ratio=1.0)\n","  temp_img = cv2.resize(temp_img,(15,15))\n","  return temp_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oGLfxgMKwJ_8"},"source":["#1æ ·æœ¬æ± \n","def sampling_unlabel(unlabeled_X,scale):\n","  pe = np.random.permutation(unlabeled_X.shape[0])\n","  unlabeled_X =unlabeled_X[pe]\n","  if len(unlabeled_X) <= scale:\n","    scale = len(unlabeled_X)\n","  unlabeled_X = unlabeled_X[0:scale,:]\n","  return unlabeled_X, pe[0:scale]\n","#2mixupç»™ä¼ªæ ‡ç­¾\n","def mixup_supervised(unlabeled,model,scale):\n","  temp_unlabeled,temp_index=sampling_unlabel(unlabeled,scale)\n","  # for i in range(len(temp_unlabeled)):\n","  plabel_all = []\n","  acc_all = []\n","  last_unlabeled = []\n","  last_index = []\n","  print('\\næ•°æ®å¢å¼ºä¸­â€¦â€¦')\n","  for i in tqdm(range(len(temp_unlabeled)),position=0):\n","    # data_enhanced=np.zeros((7,15,15,12))\n","    #å‡å°‘å¢å¼ºæ¬¡æ•°\n","    pseudo_label = []\n","    data_enhanced=np.zeros((10,15,15,12))\n","    data_enhanced[0,]=temp_unlabeled[i,]\n","    #æ°´å¹³å‚ç›´ç¿»è½¬\n","    data_enhanced[1,]=np.flip(temp_unlabeled[i,],axis=0)\n","    data_enhanced[2,]=np.flip(temp_unlabeled[i,],axis=1)\n","    #180ã€270åº¦æ—‹è½¬\n","    data_enhanced[3,]=np.rot90(temp_unlabeled[i,],k=2)\n","    data_enhanced[4,]=np.rot90(temp_unlabeled[i,],k=3)\n","    #å¢åŠ é«˜æ–¯å™ªå£°(è½»å™ªï¼Œé‡å™ª)\n","    data_enhanced[5,] = cv2.GaussianBlur(temp_unlabeled[i,], (3,3),sigmaX=5,sigmaY=5)\n","    data_enhanced[6,] = cv2.GaussianBlur(temp_unlabeled[i,], (3,3),sigmaX=0.5,sigmaY=0.5)\n","    #éšæœºè£å‰ªç¼©æ”¾\n","    data_enhanced[7,] = random_crop_resize(temp_unlabeled[i,])\n","    #éšæœºè£å‰ª\n","    data_enhanced[8,] = random_crop_pad(temp_unlabeled[i,])\n","    data_enhanced[9,] = random_crop_pad(temp_unlabeled[i,])\n","    ##############################æ˜¾ç¤º\n","    # plt.figure()\n","    # for j in range(len(data_enhanced)):\n","    #   plt.subplot(1,10,j+1)\n","    #   plt.imshow(data_enhanced[j,:,:,0])\n","    # plt.show()\n","    #å¢å¼ºç»“æœçš„embed\n","    enhance_pred=model.predict(data_enhanced)\n","    #è®¡ç®—åº¦é‡è·å–ä¼ªæ ‡ç­¾\n","    if softmax_threshold(enhance_pred,threshold=0.5) == True:\n","      pseudo_label = np.argmax(enhance_pred,axis=1)\n","      # print('éƒ½å¤§äº')\n","      counts = np.bincount(pseudo_label)\n","      plabel= np.argmax(counts)\n","      acc=counts[plabel]/len(pseudo_label)\n","      plabel_all.append(plabel)\n","      acc_all.append(acc)\n","      last_unlabeled.append(temp_unlabeled[i,])\n","      last_index.append(temp_index[i,])\n","    # else:\n","    #   temp_unlabeled=np.delete(temp_unlabeled,i,axis=0)\n","    #   temp_index=np.delete(temp_index,i)\n","  plabel_all = np.asarray(plabel_all,dtype=int)\n","  acc_all = np.asarray(acc_all,dtype=float)\n","  last_unlabeled = np.asarray(last_unlabeled)\n","  last_index = np.asarray(last_index)\n","  return last_unlabeled,last_index,plabel_all,acc_all"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SgYTBDASRJKo"},"source":["#ä½ä¸ç¡®å®šæ€§é—¨é™å‡½æ•°"]},{"cell_type":"code","metadata":{"id":"0S3MF0MIRPvg"},"source":["def get_uncertainty(h_unlabeled,t_label,h_index,model,uthreshold=0.1,times=100):\n","  uncertain_label = []\n","  all_uncertain_label = []\n","  print('\\nä¸ç¡®å®šæ€§è®¡ç®—ä¸­â€¦â€¦')\n","  for i in tqdm(range(times),position=0):\n","    templabel = model.predict(h_unlabeled)\n","    templabel = np.argmax(templabel,axis=1)\n","    uncertain_label.append(templabel)\n","  uncertain_label = np.asarray(uncertain_label)\n","  col_var = np.var(uncertain_label,axis=0)\n","  lu_unlabeled = h_unlabeled[col_var < uthreshold]\n","  lu_index = h_index[col_var < uthreshold]\n","  lu_label = t_label[col_var < uthreshold]\n","  return lu_unlabeled,lu_label,lu_index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VfoRou927mhw"},"source":["#ç²¾åº¦è¯„ä»·ä¸æ··æ·†çŸ©é˜µç”»å›¾\n"]},{"cell_type":"code","metadata":{"id":"HPYJZoVhFgDq"},"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score, cohen_kappa_score\n","def get_OA(cm):\n","    \n","    total = cm.sum()\n","    \n","    diag_arr = np.diagonal(cm, offset=0)\n","    \n","    correct = diag_arr.sum()\n","    \n","    OA = (correct/total) * 100\n","    \n","    return OA\n","\n","def get_cm_oa_kappa(Y_test, Y_pred):\n","    Y_pred = np.argmax(Y_pred, axis=1)\n","    \n","    Y_test = Y_test.tolist()\n","    Y_pred = Y_pred.tolist()\n","  \n","  \n","    cm = confusion_matrix(Y_test,Y_pred)\n","    \n","    oa = get_OA(cm)\n","    \n","    kappa = cohen_kappa_score( Y_test,Y_pred)\n","    \n","    \n","    return cm, oa, kappa\n","\n","def load_npz(npz_path):\n","    \n","    npz_arr = np.load(npz_path)\n","    _files = npz_arr.files\n","    \n","    return npz_arr[_files[0]]\n","\n","import matplotlib.pyplot as pl\n","from sklearn import metrics\n","# ç›¸å…³åº“\n","\n","def plot_matrix(y_test, y_pred, labels_name, title=None, thresh=0.8, axis_labels=None):\n","  Y_pred = np.argmax(y_pred, axis=1)\n","  \n","  y_true = y_test.tolist()\n","  Y_pred = Y_pred.tolist()\n","# åˆ©ç”¨sklearnä¸­çš„å‡½æ•°ç”Ÿæˆæ··æ·†çŸ©é˜µå¹¶å½’ä¸€åŒ–\n","  cm = confusion_matrix(y_true, Y_pred)  # ç”Ÿæˆæ··æ·†çŸ©é˜µ \n","  cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # å½’ä¸€åŒ–\n","\n","# ç”»å›¾ï¼Œå¦‚æœå¸Œæœ›æ”¹å˜é¢œè‰²é£æ ¼ï¼Œå¯ä»¥æ”¹å˜æ­¤éƒ¨åˆ†çš„cmap=pl.get_cmap('Blues')å¤„\n","  pl.imshow(cm, interpolation='nearest', cmap=pl.get_cmap('Blues'))\n","  pl.colorbar()  # ç»˜åˆ¶å›¾ä¾‹\n","  pl.title(title, fontsize=15,fontfamily='SimHei')\n","# å›¾åƒæ ‡é¢˜\n","  if title is not None:\n","      pl.title(title)\n","# ç»˜åˆ¶åæ ‡\n","  num_local = np.array(range(len(labels_name)))\n","  if axis_labels is None:\n","      axis_labels = labels_name\n","  pl.xticks(num_local, axis_labels, rotation=45, fontsize=10)  # å°†æ ‡ç­¾å°åœ¨xè½´åæ ‡ä¸Šï¼Œ å¹¶å€¾æ–œ45åº¦\n","  pl.yticks(num_local, axis_labels, fontsize=10)  # å°†æ ‡ç­¾å°åœ¨yè½´åæ ‡ä¸Š\n","  pl.ylabel('True label',fontsize=12,verticalalignment='center')\n","  pl.xlabel('Predicted label',fontsize=12,verticalalignment='center')\n","  \n","\n","# å°†ç™¾åˆ†æ¯”æ‰“å°åœ¨ç›¸åº”çš„æ ¼å­å†…ï¼Œå¤§äºthreshçš„ç”¨ç™½å­—ï¼Œå°äºçš„ç”¨é»‘å­—\n","  for i in range(np.shape(cm)[0]):\n","      for j in range(np.shape(cm)[1]):\n","          if int(cm[i][j] * 100 + 0.5) > 0:\n","              pl.text(j, i, format(int(cm[i][j] * 100), 'd') + '%',\n","                      ha=\"center\", va=\"center\",\n","                      color=\"white\" if cm[i][j] > thresh else \"black\")  # å¦‚æœè¦æ›´æ”¹é¢œè‰²é£æ ¼ï¼Œéœ€è¦åŒæ—¶æ›´æ”¹æ­¤è¡Œ\n","# æ˜¾ç¤º\n","  # pl.figure(dpi=1000) \n","  pl.savefig('/content/gdrive/MyDrive/CM/RSL/paperimage/cm_2.png',dpi=1000,bbox_inches = 'tight')\n","  # pl.savefig('/content/gdrive/MyDrive/CM/RSL/paperimage/cm.png')\n","  pl.show()\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2n4wSfTXiZsC"},"source":["labels = ['farmland', 'forest', 'grassland', 'water', 'built-up', 'bare land', 'others']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJRHzQL9npFW"},"source":["model1 = import_model('/content/gdrive/MyDrive/CM/RSL/CE_highsoftmx_highmix_lowups4.h5',train_para = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JRaAnPt66Vi6","executionInfo":{"status":"ok","timestamp":1622557161467,"user_tz":-480,"elapsed":5644,"user":{"displayName":"Boan Chen","photoUrl":"","userId":"13891223129202903233"}},"outputId":"06493e24-b2ad-4b02-c0f3-87608f55de26"},"source":["Y_pred = model1.predict(X_test, verbose=1)\n","# print(Y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["650/650 [==============================] - 6s 9ms/sample\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0D6DjUDQ_UzC","executionInfo":{"status":"ok","timestamp":1622557168284,"user_tz":-480,"elapsed":365,"user":{"displayName":"Boan Chen","photoUrl":"","userId":"13891223129202903233"}},"outputId":"291831da-f4f9-4604-8ea1-f020a04b7093"},"source":["cm, oa, kappa = get_cm_oa_kappa(Y_test, Y_pred)\n","print('acc: {:.2f}%  Kappa: {:.4f}'.format(oa,kappa))\n","print('acc: {:2f}%'.format(oa))\n","print(cm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["acc: 91.23%  Kappa: 0.8970\n","acc: 91.230769%\n","[[99  0  0  0  0  1  0]\n"," [ 1 86 12  1  0  0  0]\n"," [ 1  5 90  0  0  4  0]\n"," [ 0  0  0 94  0  6  0]\n"," [ 0  0  3  0 86 11  0]\n"," [ 0  0  3  0  1 96  0]\n"," [ 0  1  5  1  0  1 42]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B9mKI9-Jcj95","outputId":"00d6939e-dc90-4531-a97c-1e7f468abd45"},"source":["plot_matrix(Y_test ,Y_pred, [0,1,2,3,4,5,6], title='Confusion Matrix',\n","            axis_labels=labels)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["findfont: Font family ['SimHei'] not found. Falling back to DejaVu Sans.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"gzeK29DiNbDl"},"source":["#åŠç›‘ç£å‡½æ•°"]},{"cell_type":"code","metadata":{"id":"jHIpujC5LsSV"},"source":["def delete_unlabeled(u_pool,d_index):\n","  # for i in d_index:\n","  d_pool = np.delete(u_pool,d_index,axis=0)\n","  return d_pool\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DD5lT5EauAvy"},"source":["def import_model(weightpath,train_para):\n","  ksize = 2 * r + 1\n","  inputshape = L.Input((ksize,ksize,hchn))\n","  output_feature = feature_extraction_CNN(inputshape,training_parameter=train_para, n_filters=64)\n","  logits = L.Dense(NUM_CLASS+1, activation = 'softmax')(output_feature)\n","  model = K.models.Model(inputshape, logits)\n","  adam = K.optimizers.Adam(lr=1e-4,beta_1=0.9, beta_2=0.999, epsilon=None,decay=0.01)\n","  # adam = K.optimizers.Adam(lr=1e-6)\n","  model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics=['acc'])\n","  model.load_weights(weightpath)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utGY56ePrLeg"},"source":["# #é™å®šå¢åŠ ä¼ªæ ‡ç­¾æ ·æœ¬ä¸ªæ•°\n","def sampling_unlabel(unlabeled_X,scale):\n","  pe = np.random.permutation(unlabeled_X.shape[0])\n","  unlabeled_X =unlabeled_X[pe]\n","  if len(unlabeled_X) <= scale:\n","    scale = len(unlabeled_X)\n","  unlabeled_X = unlabeled_X[0:scale,:]\n","  return unlabeled_X, pe[0:scale]\n","def limited_pseudo_label(sample,plabel,index,lnumber=50):\n","  \"\"\"\n","  lnumber:é™åˆ¶çš„å•ç±»æœ€å¤§ä¸ªæ•°\n","  \"\"\"\n","  #é€ç±»ç¡®è®¤æ ·æœ¬ä¸ªæ•°\n","  least_num = Counter(plabel).most_common()[-1][1]\n","  # print(least_num)\n","  flag = 0\n","  for i in range(1,8):\n","    ti = index[plabel == i]\n","    ts = sample[plabel == i]\n","    tl = plabel[plabel == i]\n","    # print(tl)\n","    if len(ti) >= lnumber:\n","      randnum = random.sample(range(len(ti)),lnumber)\n","    else:\n","      randnum = [j for j in range(len(ti))]\n","    # print(randnum)\n","    ti = ti[randnum]\n","    ts = ts[randnum,:]\n","    tl = tl[randnum]\n","    if flag == 0:\n","      last_sample = ts\n","      last_label = tl\n","      last_index = ti\n","      flag += 1\n","    else:\n","      last_sample = np.append(last_sample,ts,axis=0)\n","      last_label = np.append(last_label,tl)\n","      last_index = np.append(last_index,ti)\n","  return last_sample,last_label,last_index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GiZ8UXSA4KMz"},"source":["#softmaxé«˜ç½®ä¿¡åº¦+é«˜æŠ—å™ªæ€§+UPSï¼ˆä¸‰é—¨æ§åŠç›‘ç£ï¼‰"]},{"cell_type":"code","metadata":{"id":"F7FUd_n3QwJA"},"source":["def train_hs_hm_lups(X,Y,X_val,Y_val,unlabeled_pool):\n","  ####è®¾ç½®å‚æ•°\n","  num_times = 5#æŠ½å–num_timesæ¬¡ï¼Œæ¯æ¬¡epochè½®\n","  X_train, Y_train = shuffle(X, Y)\n","  model_checkpoint = ModelCheckpoint('/content/gdrive/MyDrive/CM/RSL/CE_highsoftmx_highmix_lowups6.h5', monitor='val_acc', verbose=1, \n","                                       save_best_only=True, save_weights_only=True,period=3)\n","  early_stopping =EarlyStopping(monitor='val_acc', patience=20)\n","####è½½å…¥åˆå§‹æƒé‡ï¼Œè®­ç»ƒæ¨¡å¼\n"," \n","  last_X = X\n","  last_Y = Y\n","  flag = 0\n","\n","  for i in tqdm(range(num_times),position=0):\n","    #å…ˆé€‰å‡ºé«˜ç½®ä¿¡åº¦\n","    mmodel = import_model('/content/gdrive/MyDrive/CM/RSL/CE_hightsoftmax&low_UPS.h5',train_para=False)\n","    temp_unlabel,temp_plabel,temp_index = get_highsoftmax(mmodel,unlabeled_pool,5000,threshold = 0.98)\n","    #å†è®¡ç®—é«˜æŠ—å™ªæ€§\n","    temp_unlabel,temp_plabel,temp_index = highmix_unlabeled_for_softmax(temp_unlabel,temp_index,temp_plabel,mmodel,threshold=0.9)\n","    #ç„¶åè®¡ç®—ä½ä¸ç¡®å®šæ€§ï¼Œæ‰“å¼€dropout\n","    mmodel = import_model('/content/gdrive/MyDrive/CM/RSL/CE_hightsoftmax&low_UPS.h5',train_para=True)\n","    temp_unlabel,temp_plabel,temp_index = unlabel2lowups(temp_unlabel,temp_index,temp_plabel,mmodel,uthreshold=0.1,times=100)\n","    print('åˆå§‹è®­ç»ƒé›†å¤§å°ï¼š',X.shape)\n","    # print(Counter(temp_plabel))\n","    temp_unlabel,temp_plabel,temp_index = limited_pseudo_label(temp_unlabel,temp_plabel,temp_index)#å¹³è¡¡æŠ½æ ·\n","    print('æ–°å¢æ ·æœ¬ç»Ÿè®¡:',Counter(temp_plabel))\n","    unlabeled_pool = delete_unlabeled(unlabeled_pool,temp_index)#æ›´æ–°æ ·æœ¬æ± \n","    temp_plabel = to_categorical(temp_plabel,8)#åŒç±»å‹\n","    # print(last_Y.shape)\n","    last_X = np.append(last_X,temp_unlabel,axis=0)\n","    last_Y = np.append(last_Y,temp_plabel,axis=0)\n","    print('å¢åŠ åšè®­ç»ƒé›†å¤§å°ï¼š',last_X.shape)\n","    # print(last_Y.shape)\n","    mmodel.fit(x=last_X,y=last_Y,batch_size=50,epochs=100,shuffle=True,validation_data=(X_val, Y_val),\n","                        callbacks=[model_checkpoint, early_stopping])\n","    flag += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9sqeWU6_VBWd"},"source":["Y_train=to_categorical(Y_train)\n","Y_val=to_categorical(Y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4g8XhWSUGMq"},"source":["train_hs_hm_lups(X_train,Y_train,X_val,Y_val,unlabeled_pool)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETMgh45ETopq"},"source":[""],"execution_count":null,"outputs":[]}]}